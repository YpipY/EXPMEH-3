---
title: "Assignment 4 - Coordinating Heart Rate"
author: "Riccardo Fusaroli"
date: "November 6, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)


?zoo::na.spline()
Lmax
L 
RR
TT
DET
grid.expand

library(pacman)
p_load(gridExtra,lme4,evobiR,dplyr,stringr,dotCall64,crqa,zoo)
library(tidyverse)
```

## Analysing Heart Rate and Respiration data

The goal of this assignment is to first familiarize you with heart rate, and respiration data and their preprocessing. The second part explores how to analyze interpersonal coordination of these signals.

These are the questions you need to be able to answer at the end of the assignment (aka that you need to submit as part of the portfolio)

1) How do you preprocess heart rate and respiration data? Describe the process. If any data needs to be excluded, list the excluded data and motivate the exclusion.

2) Do you observe interpersonal coordination in heart rate and respiration? Describe your control baseline, the method used to quantify coordination, and the statistical models used to infer whether coordination was higher than in the baseline. Report the results of the models.

3) Do you observe differences in coordination between conditions? Report the models and results.

4) Is respiration coordination a likely driver of heart rate coordination? Describe how you would test for it. Bonus points if you actually run the tests and report methods and results.

N.B. Because of the timing, we're starting this exercise before collecting the data.
Instead, you will develop your script this week on data from two years ago (Study1) and last year (Study2).
When you hand in the assignment for feedback, you can use the old data. But when you hand in the final version for the exam, you need to adapt your script to use the data we collect next week in the lab.
(For the old data): Note that synchronouns and turn-taking are the same task across both studies, but the third condition is different: two years ago it was self-paced joint reading; last year it was tv-series conversation.

NB: For this exercise, you will need to do something very similiar to what you've done before spread over several weeks. Ie parse data, look at the plots, decide on data cleaning, build a model, and finally evaluate and interpret the results of the models. Going back and copying the approach from previous exercises will likely be a great help.

## Step by step suggestions to solve the assignment

### Exploring physiological signals

- Choose one pair (one pair, three conditions)
- Load the logs
- Produce a plot of the participants' respiration signal and a different one of the participants' HR signal (for inspecting whether the data is usable)
  N.B: remember the slides: artifacts, downsampling, scaling.
  N.B. The gridExtra::grid.arrange() function allows you to display the plots side by side. E.g. grid.arrange(plot1, plot2, plot3, ncol=3)
- Can you eye-ball which condition if any displays more physiological coordination?

- Run crqa on heart rate and respiration data (find parameters, run crqa)
- Does this tell you more than just eyeballing the plots?

```{r}

ex = read.csv("Assignment 4/data/CleanData/Study1_G1_T1_Synchronous.csv")

plot(ex$time,ex$HR1)
plot(ex$time,ex$HR2)

grid.arrange(plot1, plot2, ncol=2)



ex$time 

data.frame(Matrix(ncol = ))

colnames(ex)

ex['time']

for (col in colnames(ex)) {
  print(col)
}

ex[[1]]
```


### Systematically pre-process the data
- Loop through all the files (either with a loop or with a function), check which files should be excluded, if any, and save the pre-processed time-series. Tip: plot and visually inspect the data to figure out which should be excluded.
```{r systematic pre-processing, Laurits}
clean_file = function(filename) {
  cat("\n\n",filename,sep = '')
  currentData = read.csv(paste(path, filename, sep = "")) # Loading data
  
  # Loop for each column
  for (col in colnames(currentData)[2:length(currentData)]) {
    # A temporary variable to hold the outliers
    out = ifelse(scale(currentData[[col]]) > 2.5 |
                 scale(currentData[[col]]) < -2.5 |
                 currentData[[col]] == -10,
                 currentData[[col]], NA)
    # Removing the outliers 
    currentData[[col]] = ifelse(scale(currentData[[col]]) > sd(currentData[[col]])*2.5 |
                                scale(currentData[[col]]) < sd(currentData[[col]])*-2.5 |
                                currentData[[col]] == -10, 
                                NA, currentData[[col]])
    currentData[[col]] = na.spline(currentData[[col]])
    # Printing how many percentages of the data that was removed
    perc = round(length(na.omit(out))/length(currentData[[1]])*100,2)
    cat("\nRemoved ",perc, "% from ", col,sep='')
    
    currentData[[col]] = scale(currentData[[col]]) # Scaling
  }
  
  # Creating new dataframe
  rows = length(SlidingWindow("mean", currentData[[1]], 200, 100))
  cols = length(currentData)
  cleanData = data.frame(matrix(nrow = rows, ncol = cols)) 
  colnames(cleanData) = colnames(currentData)
  # Downsizing each column to the new dataframe
  for (col in colnames(cleanData)) {
    cleanData[[col]] = SlidingWindow("mean", currentData[[col]], 200, 100)  
  }

  return(cleanData)
}
```

```{r systematic pre-processing, using spline}


#spline tests
ex = read.csv("Assignment 4/data/CleanData/Study1_G1_T1_Synchronous.csv")

ex$Resp2 = ifelse(scale(ex$Resp2) > sd(ex$Resp2)*2.5 |
                                scale(ex$Resp2) < sd(ex$Resp2)*-2.5 |
                                ex$Resp2 == -10, 
                                NA, ex$Resp2)
ex$Resp1 = ifelse(scale(ex$Resp1) > sd(ex$Resp1)*2.5 |
                                scale(ex$Resp1) < sd(ex$Resp1)*-2.5 |
                                ex$Resp1 == -10, 
                                NA, ex$Resp1)

ex$Resp1 = na.spline(ex$Resp1,maxgap=1000)

ex = na.spline(ex,maxgap=5000)

ex<-as.data.frame(ex)

length(na.omit(Study1_G1_T1_Synchronous_clean[,2]))
length(na.omit(ex[,2]))

?na.spline
237631

# Cleaning function 
clean_file = function(filename) {
  tryCatch({
  cat("\n\n",filename,sep = '')
  currentData = read.csv(paste(path, filename, sep = "")) # Loading data
  
  # Loop for each column
  for (col in colnames(currentData)[2:length(currentData)]) {
    # A temporary variable to hold the outliers
    out = ifelse(scale(currentData[[col]]) > 2.5 |
                 scale(currentData[[col]]) < -2.5 |
                 currentData[[col]] == -10,
                 currentData[[col]], NA)
    # Removing the outliers 
    currentData[[col]] = ifelse(scale(currentData[[col]]) > sd(currentData[[col]])*2.5 |
                                scale(currentData[[col]]) < sd(currentData[[col]])*-2.5 |
                                currentData[[col]] == -10, 
                                NA, currentData[[col]])
    # Printing how many percentages of the data that was removed
    perc = round(length(na.omit(out))/length(currentData[[1]])*100,2)
    cat("\nRemoved ",perc, "% from ", col,sep='')
    
    currentData[[col]] = scale(currentData[[col]]) # Scaling
  }
  
  # Replacing outliers with a spline
  currentData = na.spline(currentData,maxgap=5000)
  currentData<-as.data.frame(currentData)
  
  # Creating new dataframe
  rows = length(SlidingWindow("mean", currentData[[1]], 200, 100))
  cols = length(currentData)
  cleanData = data.frame(matrix(nrow = rows, ncol = cols)) 
  colnames(cleanData) = colnames(currentData)
  # Downsizing each column to the new dataframe
  for (col in colnames(cleanData)) {
    cleanData[[col]] = SlidingWindow("mean", currentData[[col]], 200, 100)  
  }

  return(cleanData)
  }, error=function(e){cat("ERROR :",conditionMessage(e), "\n")})
}
    
path = "C:/Users/slmoni/Documents/Uni/Experimental Methods III/EXPMETH3/Assignment 4/data/CleanData/" # Set any path here


for (file in list.files(path = path)) {
  name = str_replace(file, ".csv","_clean")
  eval(parse(text=paste(name," = clean_file('",file,"')",sep='')))
}

View(Study1_G2_T3_SelfPaced_clean)

```


- Run crqa on all the pre-processed time-series and save the output (don't forget to add columns with study, group, condition and trial). Tip: remember to first assess optimal parameters (dimensions, delay, radius) across all timeseries. Tip: it will often fail, just take whatever parameters you get, select optimal across timeseries parameters and run crqa on all timeseries with those. Tip: double check the rr. When I ran the loop, I got very low rr, so I adjusted the radius until the average of rr across all pairs was approx. 4%.

### Creating controls: shuffled controls
 - loop through all pairs and conditions
 - shuffle the timeseries (take a timeseries and rearrange its values in a random order). Tip check the sample() function
 - run crqa and save the output. NB. which delay, embed, radius parameters should you use?
 - statistically compare the crqa indexes in real and shuffled pairs
 
### TRICKY! Creating controls: surrogate pair controls
 - Per each real pair, identify at least one surrogate pair (matching one of the participants, with somebody doing the same task, but in a different pair). Tip: Malte will share a method to do this on screen.
 - Run crqa on all the surrogate pairs and save the output. NB. which delay, embed, radius parameters should you use?
 - Test whether crqa shows a difference between real and surrogate pairs

### Testing effects of conditions
 - make a (probably underpowered) mixed model testing effects of the different conditions on heart rate and respiration coordination
 - N.B: would it make sense to include surrogate pairs? and if so how? what would that tell you?

### Effects of respiration coordination on heart rate coordination
 - describe how you would test those.
 - Optional: run the models and report them